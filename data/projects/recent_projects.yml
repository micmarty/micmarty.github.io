section_title: Recent Projects
# --------------------------------------------------------------------------------------------------------------------
first_project:
  project_title: Command-driven trajectory planning and following for a self-driving car in a simulated environment
  link: https://youtu.be/PkTxzsUUCgc
  anchor: carla-trajector-prediction
  labels:
    - CARLA simulator
    - vision-based
    - supervised
  badge: My MSc
  short_description: My master's thesis about fully autonomous agent that includes perception, planning and control
  long_description:
    - expandable: false
      paragraph: |

        My agent is able to <b>navigate from A to B</b> with respect to human input, like: turn left, right, keep lane, go straight (GPS-like navigation). The only input given for the planning module (neural network) is: 224x224 RGB image + command. 
        Output is a ~15m trajectory ahead of agent's car. Most important thing to note is that <b>predicted trajectory does not says "how to drive" but "where is the legal path that should be followed"</b>.

        Besides planning, agent can detect and react to traffic lights (simple perception module made with TensorFlow Object Detection API).

        Proposed method of <b>Reference Trajectory Approximation</b> allows for better visual understanding how agent percives the environment in the long run.
        Although the internals of this system are not that much compilcated as it may seem (in comparison with real-world systems), but it took a very long time to do research, gather data, implement and evaluate.

        <b>More videos + extras (MLinPL):</b> <a href="https://drive.google.com/open?id=1Qrm5noTVHSC7yQw0NQTOlLunUHTke4lN" target="_blank">link</a>
        Feel free to <b>contact me directly if we have common research interests</b> or my work just caught your eye üòâ.
        <!-- Seminarka PG 15.04.2019 (ENG) - milestones: <a href="https://docs.google.com/presentation/d/10EDFtKnHjQCcLoQ4SHXRuZHgmUHgD-azZSHZeRViCzE/edit?usp=sharing -->
  video_path: |
    <iframe width="100%" height="490" src="https://www.youtube.com/embed/d3xyjut2gWw?autoplay=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <!-- <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTmvj-SDdWNfiVog7QPM46hl0yPuGJNOPKxTQ2YOwWcCZHKsUk-BWXZx2XPgyCXjzWwRA9ejxzCkopq/embed?start=true&loop=true&delayms=3000" frameborder="0" width="100%" height="490" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe> -->
    <!-- <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQuGj9DRZ-LfheFwRGICpl93mzybDCpEaEPvKzPeP1J9QoaKTgFu2vTwf_Ey_8jm-nIg8apjwb0Mr5x/embed?start=true&loop=true&delayms=3000" frameborder="0" width="100%" height="490" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe> -->
  buttons:
    - name: MLinPL Poster
      link: https://drive.google.com/open?id=1pNrkshVnNiNE9TEEa6w1Rsv8v9JEYcB5
    - name: Thesis PDF (PL)
      link: https://drive.google.com/open?id=13CXULSNrVkydnqRSUOggkUtkPCJcKUOi
    - name: Project summary on YouTube (ENG)
      link: https://youtu.be/PkTxzsUUCgc
    # - Slides (PL) -> not relevant anymore
    #   https://docs.google.com/presentation/d/1tYvrb2V‚îÇ‚îÇ
# --------------------------------------------------------------------------------------------------------------------
second_project:
  project_title: CARLA Real-traffic scenarios (CRTS) and bird-eye's view visualization library <a>
  link: https://sites.google.com/view/carla-real-traffic-scenarios/home
  anchor: crts
  labels:
    - deepsense.ai
    - Volkswagen
    - open source
  badge: CRTS
  short_description: |
    <b>Real-world traffic transferred to CARLA,<br> RL agent experiments 2D Visualization tool</b>
  long_description:
    - expandable: false
      html_partial_path: partials/projects/birdview_table
      paragraph: |
        We have transferred the data of real-world car traffic to the simulation.
        The released scenarios cover maneuvers like overtaking on US freeways or exiting roundabouts.

        Datasets that were used:
        <ul>
           <li><a href="https://arxiv.org/abs/2007.08463">openDD (roundabouts)</a></li>
           <li><a href="https://www.fhwa.dot.gov/publications/research/operations/07030/">NGSIM (US freeways)</a></li>
        </ul>

        Our research resulted not only in creating the benchamark scenarios but also <b>evaluating different RL policies</b> (reward schemes) and <b>observation types</b>: 
        <ul>
          <li>bird's-eye view with ablations
            <ul>
              <li>front visibility only</li>
              <li>front+read visibility, without centerline</li>
              <li>fron+read visibility + framestack (memory)</li>
            </ul>
          </li>
          <li>RGB camera</li>
          <li>RGB camera + LIDAR</li>
        </ul>

        Experimentally, we find that bird's-eye view and dense rewards combined with a penalty for a failure to complete an episode <b>generalizes best</b> to validation scenarios.
        <br><br>

        My role in the project:
        <ul>
          <li>Main author of the <a href='https://pypi.org/project/carla-birdeye-view/' target='_blank'>bird-eye's visualization library on PyPI</a> (<a href='https://github.com/deepsense-ai/carla-birdeye-view' target='_blank'>Github repo</a>)
          <li>Transfer of real-world maps for CARLA (UE4) using <a href="https://www.mathworks.com/products/roadrunner.html" target='_blank'>RoadRunner</a>
          <li>Co-author of an <a href="https://www.ont.com.pl/wp-content/uploads/artykuly/Lane_change_CoRL.pdf" target='_blank'>article</a> accepted to the <a href="https://deepsense.ai/deepsense-ai-research-presented-at-neurips-2020/" target='_blank'>Autonomous Driving Workshop</a> at NeurIPS 2020 (<a href='https://arxiv.org/abs/2012.11329.pdf' target='_blank'>arxiv version</a>)</li>
        </ul>
        <b>Extensive description can be found <a href="https://deepsense.ai/deepsense-ai-research-presented-at-neurips-2020/">in this blog post</a></b>.
        <br>
        <b>Credits</b>: B≈Ça≈ºej Osi≈Ñski, Piotr Mi≈Ço≈õ, Adam Jakubowski, Pawe≈Ç Ziƒôcina, Micha≈Ç Martyniak, Christopher Galias, Antonia Breuer, Silviu Homoceanu, Henryk MichalewskiB≈Ça≈ºej Osi≈Ñski, Piotr Mi≈Ço≈õ, Adam Jakubowski, Pawe≈Ç Ziƒôcina, Krzysztof Galias, Henryk Michalewski.

  image_path: images/projects/recent/ngsim-full.gif
  buttons:
    - name: Birdview - PyPI package
      link: https://pypi.org/project/carla-birdeye-view/
    - name: Birdview - Source Code
      link: https://github.com/deepsense-ai/carla-birdeye-view
    - name: RL Research results
      link: https://sites.google.com/view/carla-real-traffic-scenarios/home
    - name: Paper on arxiv
      link: https://arxiv.org/abs/2012.11329
# --------------------------------------------------------------------------------------------------------------------
third_project:
  project_title: TensorHive
  anchor: tensorhive
  labels:
    - nvidia-smi
    - open source
  badge: false
  short_description: Lightweight multi-node Nvidia GPU reservation guard.
  long_description:
    - expandable: false
      html_partial_path: partials/projects/tensorhive_table
      paragraph: |
        TensorHive functionalities are as follows:
        <ul>
          <li>Users can use calendar view to make a reservation for a particular set GPUs</li>
          <li>Reserved resources will be protected from running other user's processes</li>
          <li>If reservation gets violeted, the intruder (+admin) will be notified </li>
          <li>It can monitor node resources like: GPU, memory load, etc.</li>
        </ul>
        The project has started within <a href="http://gradient.eti.pg.gda.pl/en/" target="_blank"><img src="images/gradient_logo_small.png" style="margin-bottom: 0px"><b>GradientPG</b></a> (Student Research Group)
  image_path: images/projects/major/tensorhive_dashboard_multigpu.png
  link: https://github.com/roscisz/TensorHive
  buttons:
    - name: Source Code
      link: https://github.com/roscisz/TensorHive
    - name: Article (PL)
      link: https://gradient.eti.pg.gda.pl/projekty/2017/10/07/projekt-tensor-hive.html
    - name: Article (ENG)
      link: https://gradient.eti.pg.gda.pl/en/projects/2017/10/07/projekt-tensor-hive.html
# --------------------------------------------------------------------------------------------------------------------
fourth_project:
  project_title: Audio signal analysis in Python with Librosa
  anchor: audio
  labels:
    - audio
    - own research
  badge: hobby
  short_description: |
    I had to do some research about choosing the best model for my neural network.
    My journey started with librosa - it can transform an audio excerpt into many visual representations, 
    e.g. spectogram or chroma and has very useful features like beat detection.
  long_description:
    - expandable: false
      paragraph: "" # NO DESCRIPTION

  image_path: images/projects/major/librosa_chroma.png
  link: https://github.com/micmarty/audio-preprocessing-exercises-with-librosa/blob/master/librosa_sandbox_1.ipynb
  buttons:
    - name: Source Code
      link: https://github.com/micmarty/audio-preprocessing-exercises-with-librosa/blob/master/librosa_sandbox_1.ipynb
